{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5434f8b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11078,
     "status": "ok",
     "timestamp": 1744203363775,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "GxbJN0v4VLmy",
    "outputId": "0ae30ed1-e431-4018-9701-aac1f392673e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5df201f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20621,
     "status": "ok",
     "timestamp": 1744203394292,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "GzywvNt2ZiPY",
    "outputId": "f423c500-0426-41f5-cb75-1ce6eb645163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1a0a3",
   "metadata": {
    "id": "TbwT1kTaV_iq"
   },
   "source": [
    "**We will copy from our repository the functions we need!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3abb2a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744204617578,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "XSa34A_GV-IU"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import html\n",
    "import os\n",
    "\n",
    "#from config import DATA_PATH #load path for data files in the file config\n",
    "DATA_PATH = \"/content/drive/MyDrive/NLP_PROJECT2\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEPARATOR = \"\\t\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove extra quotes from text files and html entities\n",
    "    Argumentss:\n",
    "        text (str): a string of text\n",
    "\n",
    "    Returns: (str): the \"cleaned\" text\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.rstrip() #removes spaces, tabs, new lines etc FROM THE END of the text\n",
    "\n",
    "    if '\"\"' in text: #if we have useless double \"  we clean them (\"\"somthing\"\")\n",
    "        if text[0] == text[-1] == '\"':\n",
    "            text = text[1:-1] #take the string without the first and last char (remove \")\n",
    "        text = text.replace('\\\\\"\"', '\"')\n",
    "        text = text.replace('\"\"', '\"') #double \"\"  replaced by \"\n",
    "\n",
    "    text = text.replace('\\\\\"\"', '\"')\n",
    "\n",
    "    text = html.unescape(text) #convert HTML entities into characters ex. &lt; → <\n",
    "    text = ' '.join(text.split()) #splits the text ignoring many spaces and ' '.join()\n",
    "                                  # joins them with only one space\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_file(file):\n",
    "    \"\"\"\n",
    "    Read a file and return a dictionary of the data, in the format:\n",
    "    tweet_id:{sentiment, text}\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "    lines = open(file, \"r\", encoding=\"utf-8\").readlines() #read all lines of the file (utf-8 for special chars)\n",
    "    for _, line in enumerate(lines):\n",
    "        columns = line.rstrip().split(SEPARATOR) #seperator = \\t =  tab so we get each column value which is seperated by tab\n",
    "        tweet_id = columns[0]\n",
    "        sentiment = columns[1]\n",
    "        text = columns[2:] #everything from the 3rd column and after (text)\n",
    "        text = clean_text(\" \".join(text)) #we set text as an element not a list of elements and clean it\n",
    "                                          #in other words text becomes a string instead of a list\n",
    "        data[tweet_id] = (sentiment, text) #(emotion, text)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_from_dir(path):\n",
    "    \"\"\"\n",
    "    Searches for all the .tsv and .txt files in a folder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #search inside folders and subfolders\n",
    "    files = glob.glob(path + \"/**/*.tsv\", recursive=True)\n",
    "    files.extend(glob.glob(path + \"/**/*.txt\", recursive=True))\n",
    "\n",
    "    data = {}  # use dict, in order to avoid having duplicate tweets (same id)\n",
    "               #dictionary will replace the duplicate key\n",
    "    for file in files:\n",
    "        file_data = parse_file(file)\n",
    "        data.update(file_data)\n",
    "    return list(data.values()) #list of tuples (sentiment, text) --> dont care about the keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_Semeval2017A():\n",
    "    \"\"\"\n",
    "    Loads data from dataset Semeval2017A\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    train = load_from_dir(os.path.join(DATA_PATH, \"Semeval2017A/train_dev\"))\n",
    "    test = load_from_dir(os.path.join(DATA_PATH, \"Semeval2017A/gold\"))\n",
    "\n",
    "    X_train = [x[1] for x in train]\n",
    "    y_train = [x[0] for x in train]\n",
    "    X_test = [x[1] for x in test]\n",
    "    y_test = [x[0] for x in test]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_MR():\n",
    "    with open(os.path.join(DATA_PATH, \"MR/rt-polarity.pos\"), encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        pos = f.readlines()\n",
    "\n",
    "    with open(os.path.join(DATA_PATH, \"MR/rt-polarity.neg\"), encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        neg = f.readlines()\n",
    "\n",
    "    pos = [x.strip() for x in pos]\n",
    "    neg = [x.strip() for x in neg]\n",
    "\n",
    "    pos_labels = [\"positive\"] * len(pos)\n",
    "    neg_labels = [\"negative\"] * len(neg)\n",
    "\n",
    "    split = 5000\n",
    "\n",
    "    X_train = pos[:split] + neg[:split]\n",
    "    y_train = pos_labels[:split] + neg_labels[:split]\n",
    "\n",
    "    X_test = pos[split:] + neg[split:]\n",
    "    y_test = pos_labels[split:] + neg_labels[split:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de58e6",
   "metadata": {
    "id": "CjDTB7m0Yl3i"
   },
   "source": [
    "# MAIN CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f73be90",
   "metadata": {
    "executionInfo": {
     "elapsed": 21436,
     "status": "ok",
     "timestamp": 1744204648108,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "-JtW8a4OndBz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f71f723",
   "metadata": {
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1744206395751,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "Bp_cNVaDjXUV"
   },
   "outputs": [],
   "source": [
    "# ==== METRICS ====\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# ==== TOKENIZATION ====\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "def prepare_dataset(X, y):\n",
    "    return Dataset.from_dict({'text': X, 'label': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d611ddb",
   "metadata": {
    "id": "eVv8I93ZkZqX"
   },
   "source": [
    "# **We will start with MR dataset.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c93684",
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1744204665935,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "zGexbE8uVppf"
   },
   "outputs": [],
   "source": [
    "# ==== LOAD DATA ====\n",
    "DATASET = \"MR\"\n",
    "X_train, y_train, X_test, y_test = load_MR()\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(list(set(y_train)))\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "# Prepare datasets\n",
    "train_set = prepare_dataset(X_train, y_train)\n",
    "test_set = prepare_dataset(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da666a00",
   "metadata": {
    "id": "pWBr8l9Xlm2r"
   },
   "source": [
    "**First model: siebert/sentiment-roberta-large-english**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a346ac38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 922957,
     "status": "ok",
     "timestamp": 1744205593547,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "IG3No4DrkCsJ",
    "outputId": "4225709c-3e4d-4b99-a6a4-065c5cda7cfd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9154823f646c482bbe35795f82fb103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12846ed018504a3481f64d414fbf80b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 14:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.676394</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.647820</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.345500</td>\n",
       "      <td>0.666962</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.324012</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.403266</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: siebert/sentiment-roberta-large-english\n",
      "\n",
      "Final Test Accuracy: 0.8625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'siebert/sentiment-roberta-large-english'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5058480",
   "metadata": {
    "id": "aHabUqQCtW1-"
   },
   "source": [
    "**Second model: distilbert-base-uncased-finetuned-sst-2-english**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085b0b76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 115849,
     "status": "ok",
     "timestamp": 1744205801441,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "KUxagGBAtKW-",
    "outputId": "43fe821f-06f2-4e27-aa1c-8c7ecef66095"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80df56a82fa743b1b4595aa34a3167bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee43e2c05e944b9be7e87040a0d3b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfaae726fb248329db38dc1fd5e3dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c6fb7de2c43f68997f358f14fa95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0af7f9c3df34825877b0647be5d7e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c46a6e134da4eb58d9061cd8de9eda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.397851</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.451669</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.547714</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.595341</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.606520</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "\n",
      "Final Test Accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1bd84",
   "metadata": {
    "id": "DPhHnBf7takZ"
   },
   "source": [
    "Third model: textattack/bert-base-uncased-SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbd37af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 240717,
     "status": "ok",
     "timestamp": 1744206042186,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "5Picw3gJtnv1",
    "outputId": "abf80306-3372-4496-fffc-a4bd21b01f91"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef861376be04c16af3e20f53811af48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1deb2a9c3d4b89b627ac84264e5100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51d743cc114413b4f5e8b504488ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38cd6b7e2984c789029e600fa68ad13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703fbecfd5b14cb7b8e29b9b7fd9362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900a7b9c67684d63a2e66d1ae5bbfc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61e0fff712f451f9dbd64f7efa0cc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b69258a74cb46f6b1d9e9efad201149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 03:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>0.482360</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.408721</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.478676</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.576896</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.592327</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: textattack/bert-base-uncased-SST-2\n",
      "\n",
      "Final Test Accuracy: 0.8837\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'textattack/bert-base-uncased-SST-2'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710540ac",
   "metadata": {
    "id": "0jJhJXb9vY60"
   },
   "source": [
    "# **We will continue with Semeval dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7a1bd7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7353,
     "status": "ok",
     "timestamp": 1744206049562,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "9uViSn9ajyrG"
   },
   "outputs": [],
   "source": [
    "DATASET = \"Semeval2017A\"\n",
    "X_train, y_train, X_test, y_test = load_Semeval2017A()\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(list(set(y_train)))\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "# Prepare datasets\n",
    "train_set = prepare_dataset(X_train, y_train)\n",
    "test_set = prepare_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e36dd2",
   "metadata": {
    "id": "hZbDSn-Rv6Ji"
   },
   "source": [
    "**First model: cardiffnlp/twitter-roberta-base-sentiment**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3db639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 142260,
     "status": "ok",
     "timestamp": 1744206544026,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "TH2xHSnGvy_1",
    "outputId": "a990a7e3-8d89-48e5-ab1b-370c52760bc0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9931d0bf4a37471aafba9a4e434efb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5563723623d4868b45b273c4375906e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.721568</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>1.054714</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>1.753020</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>2.084121</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>2.048360</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: cardiffnlp/twitter-roberta-base-sentiment\n",
      "\n",
      "Final Test Accuracy: 0.6903\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ddfeb",
   "metadata": {
    "id": "JChilhRMwBp5"
   },
   "source": [
    "**Second model: finiteautomata/bertweet-base-sentiment-analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b172da8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 172622,
     "status": "ok",
     "timestamp": 1744206769641,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "BHEkaP8FwCC3",
    "outputId": "995bd808-54c7-4032-844e-a5ebd7ef4427"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f423bf50034e14b537ba5d13ebac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/338 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb41d0047e448d6be080489cdfc6d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce32b7994d64031ace33892fb248ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62559b43989f436fb3706c773ec95694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49972f3ac5f744d1a380c12416105ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38b1ba745464be9aab32cd4b7296476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0372eec0f5c34361be30abc7c965093a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4151c842be4742d7961143777351ed84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e466e5a11794862a945f8ceef65f7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cfba6ef3824a7dbe46977fdb0d87bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.943131</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.682151</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>1.058799</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>1.020295</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.042668</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: finiteautomata/bertweet-base-sentiment-analysis\n",
      "\n",
      "Final Test Accuracy: 0.7077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85d733",
   "metadata": {
    "id": "itTpTH8GwTLr"
   },
   "source": [
    "**Third model: yiyanghkust/finbert-tone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "223add6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "executionInfo": {
     "elapsed": 137346,
     "status": "ok",
     "timestamp": 1744206907033,
     "user": {
      "displayName": "Νίκος Κατσαϊδώνης",
      "userId": "13489937806515013970"
     },
     "user_tz": -180
    },
    "id": "TYf0M142wa6X",
    "outputId": "1487b297-a86f-416a-c201-e11abf25ee9c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875dffa940fc4e53a0f9b4eddffbfd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a4031c3bd5411aada5920da2a7a202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83401ad2edc04d17b0cac59b599c859d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c507afe00a5d4a9bac3111448bbe4635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911f268e7997495cb66424ad23325626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba48edbe428245e3a634336a8fed1c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.824800</td>\n",
       "      <td>1.337254</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.073500</td>\n",
       "      <td>1.671640</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>1.825149</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>2.011916</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>2.121940</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: yiyanghkust/finbert-tone\n",
      "\n",
      "Final Test Accuracy: 0.4141\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = 'yiyanghkust/finbert-tone'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #we asked for API key to have wandb -> visualization of training loss\n",
    "                                      #so we disable it\n",
    "\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "# (Optional) Subsample για debug\n",
    "n_samples = 100\n",
    "small_train_dataset = tokenized_train_set.shuffle(seed=42).select(range(n_samples))\n",
    "small_eval_dataset = tokenized_test_set.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "\n",
    "# ==== TRAINING SETUP ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== FINAL EVALUATION ON FULL TEST SET ====\n",
    "predictions = trainer.predict(tokenized_test_set)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(f\"Model: {PRETRAINED_MODEL}\")\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyODN6tMqf1YopN3EIgeklgT",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
